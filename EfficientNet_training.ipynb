{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "BATCH_SIZE = 4\n",
    "EPOCHS = 30\n",
    "LEARNING_RATE = 0.001\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset paths\n",
    "DATA_DIR = \"/home/mahdi/Phishing_Project/datasets\"  # Dataset with logo folders\n",
    "OUTPUT_DIR = \"/home/mahdi/Phishing_Project/trained_model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Transforms\n",
    "data_transforms = {\n",
    "    \"train\": transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(10),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    \"val\": transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Dataset\n",
    "dataset = datasets.ImageFolder(DATA_DIR, transform=data_transforms[\"train\"])\n",
    "train_idx, val_idx = train_test_split(list(range(len(dataset))), test_size=0.2, random_state=42)\n",
    "train_dataset = torch.utils.data.Subset(dataset, train_idx)\n",
    "val_dataset = torch.utils.data.Subset(dataset, val_idx)\n",
    "\n",
    "dataloaders = {\n",
    "    \"train\": DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True),\n",
    "    \"val\": DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False),\n",
    "}\n",
    "dataset_sizes = {\"train\": len(train_dataset), \"val\": len(val_dataset)}\n",
    "class_names = dataset.classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Pretrained EfficientNet\n",
    "model = models.efficientnet_b0(weights=models.EfficientNet_B0_Weights.IMAGENET1K_V1)\n",
    "num_features = model.classifier[1].in_features\n",
    "model.classifier[1] = nn.Linear(num_features, len(class_names))  # Replace final layer for custom classes\n",
    "model = model.to(DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    " #Loss and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Function\n",
    "def train_model(model, dataloaders, criterion, optimizer, num_epochs=EPOCHS):\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        print(\"-\" * 10)\n",
    "\n",
    "        for phase in [\"train\", \"val\"]:\n",
    "            if phase == \"train\":\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with torch.set_grad_enabled(phase == \"train\"):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    if phase == \"train\":\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print(f\"{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}\")\n",
    "\n",
    "        print()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mahdi/Phishing_Project/.venv/lib/python3.12/site-packages/PIL/Image.py:1054: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.5512 Acc: 0.7901\n",
      "val Loss: 0.2595 Acc: 0.9048\n",
      "\n",
      "Epoch 2/30\n",
      "----------\n",
      "train Loss: 0.4071 Acc: 0.8395\n",
      "val Loss: 0.4424 Acc: 0.7143\n",
      "\n",
      "Epoch 3/30\n",
      "----------\n",
      "train Loss: 0.3725 Acc: 0.8642\n",
      "val Loss: 0.8003 Acc: 0.7619\n",
      "\n",
      "Epoch 4/30\n",
      "----------\n",
      "train Loss: 0.4172 Acc: 0.8272\n",
      "val Loss: 0.2804 Acc: 0.9048\n",
      "\n",
      "Epoch 5/30\n",
      "----------\n",
      "train Loss: 0.2839 Acc: 0.9136\n",
      "val Loss: 0.2161 Acc: 0.9048\n",
      "\n",
      "Epoch 6/30\n",
      "----------\n",
      "train Loss: 0.0868 Acc: 0.9877\n",
      "val Loss: 0.0405 Acc: 1.0000\n",
      "\n",
      "Epoch 7/30\n",
      "----------\n",
      "train Loss: 0.1625 Acc: 0.9753\n",
      "val Loss: 0.4959 Acc: 0.9048\n",
      "\n",
      "Epoch 8/30\n",
      "----------\n",
      "train Loss: 0.2670 Acc: 0.9012\n",
      "val Loss: 0.2600 Acc: 0.9524\n",
      "\n",
      "Epoch 9/30\n",
      "----------\n",
      "train Loss: 0.1446 Acc: 0.9383\n",
      "val Loss: 0.3306 Acc: 0.8095\n",
      "\n",
      "Epoch 10/30\n",
      "----------\n",
      "train Loss: 0.1349 Acc: 0.9630\n",
      "val Loss: 0.3232 Acc: 0.8571\n",
      "\n",
      "Epoch 11/30\n",
      "----------\n",
      "train Loss: 0.2246 Acc: 0.9506\n",
      "val Loss: 0.6845 Acc: 0.8571\n",
      "\n",
      "Epoch 12/30\n",
      "----------\n",
      "train Loss: 0.5143 Acc: 0.8395\n",
      "val Loss: 0.1790 Acc: 0.9048\n",
      "\n",
      "Epoch 13/30\n",
      "----------\n",
      "train Loss: 0.1836 Acc: 0.9012\n",
      "val Loss: 0.4956 Acc: 0.8571\n",
      "\n",
      "Epoch 14/30\n",
      "----------\n",
      "train Loss: 0.1993 Acc: 0.9259\n",
      "val Loss: 0.4408 Acc: 0.8095\n",
      "\n",
      "Epoch 15/30\n",
      "----------\n",
      "train Loss: 0.0603 Acc: 0.9877\n",
      "val Loss: 0.2416 Acc: 0.9048\n",
      "\n",
      "Epoch 16/30\n",
      "----------\n",
      "train Loss: 0.0976 Acc: 0.9630\n",
      "val Loss: 0.3377 Acc: 0.8571\n",
      "\n",
      "Epoch 17/30\n",
      "----------\n",
      "train Loss: 0.0890 Acc: 0.9753\n",
      "val Loss: 0.3890 Acc: 0.9048\n",
      "\n",
      "Epoch 18/30\n",
      "----------\n",
      "train Loss: 0.0351 Acc: 1.0000\n",
      "val Loss: 0.0879 Acc: 1.0000\n",
      "\n",
      "Epoch 19/30\n",
      "----------\n",
      "train Loss: 0.0168 Acc: 1.0000\n",
      "val Loss: 0.1057 Acc: 1.0000\n",
      "\n",
      "Epoch 20/30\n",
      "----------\n",
      "train Loss: 0.0077 Acc: 1.0000\n",
      "val Loss: 0.1297 Acc: 1.0000\n",
      "\n",
      "Epoch 21/30\n",
      "----------\n",
      "train Loss: 0.0029 Acc: 1.0000\n",
      "val Loss: 0.0673 Acc: 1.0000\n",
      "\n",
      "Epoch 22/30\n",
      "----------\n",
      "train Loss: 0.0013 Acc: 1.0000\n",
      "val Loss: 0.0701 Acc: 0.9524\n",
      "\n",
      "Epoch 23/30\n",
      "----------\n",
      "train Loss: 0.0045 Acc: 1.0000\n",
      "val Loss: 0.0507 Acc: 0.9524\n",
      "\n",
      "Epoch 24/30\n",
      "----------\n",
      "train Loss: 0.0016 Acc: 1.0000\n",
      "val Loss: 0.0340 Acc: 1.0000\n",
      "\n",
      "Epoch 25/30\n",
      "----------\n",
      "train Loss: 0.0012 Acc: 1.0000\n",
      "val Loss: 0.0527 Acc: 0.9524\n",
      "\n",
      "Epoch 26/30\n",
      "----------\n",
      "train Loss: 0.0020 Acc: 1.0000\n",
      "val Loss: 0.0273 Acc: 1.0000\n",
      "\n",
      "Epoch 27/30\n",
      "----------\n",
      "train Loss: 0.0016 Acc: 1.0000\n",
      "val Loss: 0.0324 Acc: 1.0000\n",
      "\n",
      "Epoch 28/30\n",
      "----------\n",
      "train Loss: 0.0008 Acc: 1.0000\n",
      "val Loss: 0.0358 Acc: 1.0000\n",
      "\n",
      "Epoch 29/30\n",
      "----------\n",
      "train Loss: 0.0030 Acc: 1.0000\n",
      "val Loss: 0.0188 Acc: 1.0000\n",
      "\n",
      "Epoch 30/30\n",
      "----------\n",
      "train Loss: 0.0005 Acc: 1.0000\n",
      "val Loss: 0.0098 Acc: 1.0000\n",
      "\n",
      "Model training complete and saved.\n"
     ]
    }
   ],
   "source": [
    "# Train and Save Model\n",
    "model = train_model(model, dataloaders, criterion, optimizer)\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "torch.save(model.state_dict(), os.path.join(OUTPUT_DIR, \"efficientnet_finetuned_1.pth\"))\n",
    "print(\"Model training complete and saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similarity Function\n",
    "def get_image_embedding(image_path, model, transform):\n",
    "    model.eval()\n",
    "    img = Image.open(image_path).convert(\"RGB\")\n",
    "    img = transform(img).unsqueeze(0).to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        features = model.extract_features(img)\n",
    "    return features.squeeze().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_most_similar(input_image_path, valid_images_folder, model, transform):\n",
    "    input_embedding = get_image_embedding(input_image_path, model, transform)\n",
    "\n",
    "    most_similar_image = None\n",
    "    highest_similarity = -1\n",
    "\n",
    "    for valid_image_name in os.listdir(valid_images_folder):\n",
    "        valid_image_path = os.path.join(valid_images_folder, valid_image_name)\n",
    "        valid_embedding = get_image_embedding(valid_image_path, model, transform)\n",
    "        similarity = cosine_similarity(\n",
    "            [input_embedding], [valid_embedding]\n",
    "        )[0][0]\n",
    "\n",
    "        if similarity > highest_similarity:\n",
    "            highest_similarity = similarity\n",
    "            most_similar_image = valid_image_name\n",
    "\n",
    "    return most_similar_image, highest_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'EfficientNet' object has no attribute 'extract_features'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 11\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# valid_img_path = '/home/mahdi/Phishing_Project/Valid_images/'\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# input_image_path = \"/path/to/input/logo.jpg\"\u001b[39;00m\n\u001b[1;32m     10\u001b[0m valid_images_folder \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/home/mahdi/Phishing_Project/Valid_images/\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 11\u001b[0m most_similar, similarity_score \u001b[38;5;241m=\u001b[39m \u001b[43mfind_most_similar\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_image_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_images_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_transforms\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mval\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMost similar image: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmost_similar\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with similarity score: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msimilarity_score\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[11], line 2\u001b[0m, in \u001b[0;36mfind_most_similar\u001b[0;34m(input_image_path, valid_images_folder, model, transform)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfind_most_similar\u001b[39m(input_image_path, valid_images_folder, model, transform):\n\u001b[0;32m----> 2\u001b[0m     input_embedding \u001b[38;5;241m=\u001b[39m \u001b[43mget_image_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_image_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     most_similar_image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     highest_similarity \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n",
      "Cell \u001b[0;32mIn[10], line 8\u001b[0m, in \u001b[0;36mget_image_embedding\u001b[0;34m(image_path, model, transform)\u001b[0m\n\u001b[1;32m      5\u001b[0m img \u001b[38;5;241m=\u001b[39m transform(img)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m----> 8\u001b[0m     features \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_features\u001b[49m(img)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m features\u001b[38;5;241m.\u001b[39msqueeze()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[0;32m~/Phishing_Project/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1931\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1929\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1930\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1931\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m   1932\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1933\u001b[0m )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'EfficientNet' object has no attribute 'extract_features'"
     ]
    }
   ],
   "source": [
    "# Example: Find most similar logo\n",
    "# 'BM_LOGO-04.png'\n",
    "img1 = 'test_2/309_21.jpg'\n",
    "valid_img = ['BM_LOGO-00.png' , 'BM_LOGO-01.png' ,  'BM_LOGO-02.png' , 'BM_LOGO-03.png' , 'BM_LOGO-04.png','BM_LOGO-05.png']\n",
    "input_image_path = f\"/home/mahdi/Phishing_Project/images/{img1}\"\n",
    "\n",
    "# valid_img_path = '/home/mahdi/Phishing_Project/Valid_images/'\n",
    "\n",
    "# input_image_path = \"/path/to/input/logo.jpg\"\n",
    "valid_images_folder = '/home/mahdi/Phishing_Project/Valid_images/'\n",
    "most_similar, similarity_score = find_most_similar(input_image_path, valid_images_folder, model, data_transforms[\"val\"])\n",
    "print(f\"Most similar image: {most_similar} with similarity score: {similarity_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4898/3168755010.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path, map_location=torch.device(\"cpu\")))\n"
     ]
    }
   ],
   "source": [
    "# Define the model architecture with a matching classifier\n",
    "model = models.efficientnet_b0(weights=None)  # Initialize without pre-trained weights\n",
    "num_classes = 2  # Replace with the number of classes you trained on\n",
    "model.classifier[1] = torch.nn.Linear(model.classifier[1].in_features, num_classes)\n",
    "\n",
    "# Load the saved weights\n",
    "model_path = \"/home/mahdi/Phishing_Project/trained_model/efficientnet_finetuned.pth\"\n",
    "model.load_state_dict(torch.load(model_path, map_location=torch.device(\"cpu\")))\n",
    "model.eval()  # Set model to evaluation mode\n",
    "model = model.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['features.0.0.weight', 'features.0.1.weight', 'features.0.1.bias', 'features.0.1.running_mean', 'features.0.1.running_var', 'features.0.1.num_batches_tracked', 'features.1.0.block.0.0.weight', 'features.1.0.block.0.1.weight', 'features.1.0.block.0.1.bias', 'features.1.0.block.0.1.running_mean', 'features.1.0.block.0.1.running_var', 'features.1.0.block.0.1.num_batches_tracked', 'features.1.0.block.1.fc1.weight', 'features.1.0.block.1.fc1.bias', 'features.1.0.block.1.fc2.weight', 'features.1.0.block.1.fc2.bias', 'features.1.0.block.2.0.weight', 'features.1.0.block.2.1.weight', 'features.1.0.block.2.1.bias', 'features.1.0.block.2.1.running_mean', 'features.1.0.block.2.1.running_var', 'features.1.0.block.2.1.num_batches_tracked', 'features.2.0.block.0.0.weight', 'features.2.0.block.0.1.weight', 'features.2.0.block.0.1.bias', 'features.2.0.block.0.1.running_mean', 'features.2.0.block.0.1.running_var', 'features.2.0.block.0.1.num_batches_tracked', 'features.2.0.block.1.0.weight', 'features.2.0.block.1.1.weight', 'features.2.0.block.1.1.bias', 'features.2.0.block.1.1.running_mean', 'features.2.0.block.1.1.running_var', 'features.2.0.block.1.1.num_batches_tracked', 'features.2.0.block.2.fc1.weight', 'features.2.0.block.2.fc1.bias', 'features.2.0.block.2.fc2.weight', 'features.2.0.block.2.fc2.bias', 'features.2.0.block.3.0.weight', 'features.2.0.block.3.1.weight', 'features.2.0.block.3.1.bias', 'features.2.0.block.3.1.running_mean', 'features.2.0.block.3.1.running_var', 'features.2.0.block.3.1.num_batches_tracked', 'features.2.1.block.0.0.weight', 'features.2.1.block.0.1.weight', 'features.2.1.block.0.1.bias', 'features.2.1.block.0.1.running_mean', 'features.2.1.block.0.1.running_var', 'features.2.1.block.0.1.num_batches_tracked', 'features.2.1.block.1.0.weight', 'features.2.1.block.1.1.weight', 'features.2.1.block.1.1.bias', 'features.2.1.block.1.1.running_mean', 'features.2.1.block.1.1.running_var', 'features.2.1.block.1.1.num_batches_tracked', 'features.2.1.block.2.fc1.weight', 'features.2.1.block.2.fc1.bias', 'features.2.1.block.2.fc2.weight', 'features.2.1.block.2.fc2.bias', 'features.2.1.block.3.0.weight', 'features.2.1.block.3.1.weight', 'features.2.1.block.3.1.bias', 'features.2.1.block.3.1.running_mean', 'features.2.1.block.3.1.running_var', 'features.2.1.block.3.1.num_batches_tracked', 'features.3.0.block.0.0.weight', 'features.3.0.block.0.1.weight', 'features.3.0.block.0.1.bias', 'features.3.0.block.0.1.running_mean', 'features.3.0.block.0.1.running_var', 'features.3.0.block.0.1.num_batches_tracked', 'features.3.0.block.1.0.weight', 'features.3.0.block.1.1.weight', 'features.3.0.block.1.1.bias', 'features.3.0.block.1.1.running_mean', 'features.3.0.block.1.1.running_var', 'features.3.0.block.1.1.num_batches_tracked', 'features.3.0.block.2.fc1.weight', 'features.3.0.block.2.fc1.bias', 'features.3.0.block.2.fc2.weight', 'features.3.0.block.2.fc2.bias', 'features.3.0.block.3.0.weight', 'features.3.0.block.3.1.weight', 'features.3.0.block.3.1.bias', 'features.3.0.block.3.1.running_mean', 'features.3.0.block.3.1.running_var', 'features.3.0.block.3.1.num_batches_tracked', 'features.3.1.block.0.0.weight', 'features.3.1.block.0.1.weight', 'features.3.1.block.0.1.bias', 'features.3.1.block.0.1.running_mean', 'features.3.1.block.0.1.running_var', 'features.3.1.block.0.1.num_batches_tracked', 'features.3.1.block.1.0.weight', 'features.3.1.block.1.1.weight', 'features.3.1.block.1.1.bias', 'features.3.1.block.1.1.running_mean', 'features.3.1.block.1.1.running_var', 'features.3.1.block.1.1.num_batches_tracked', 'features.3.1.block.2.fc1.weight', 'features.3.1.block.2.fc1.bias', 'features.3.1.block.2.fc2.weight', 'features.3.1.block.2.fc2.bias', 'features.3.1.block.3.0.weight', 'features.3.1.block.3.1.weight', 'features.3.1.block.3.1.bias', 'features.3.1.block.3.1.running_mean', 'features.3.1.block.3.1.running_var', 'features.3.1.block.3.1.num_batches_tracked', 'features.4.0.block.0.0.weight', 'features.4.0.block.0.1.weight', 'features.4.0.block.0.1.bias', 'features.4.0.block.0.1.running_mean', 'features.4.0.block.0.1.running_var', 'features.4.0.block.0.1.num_batches_tracked', 'features.4.0.block.1.0.weight', 'features.4.0.block.1.1.weight', 'features.4.0.block.1.1.bias', 'features.4.0.block.1.1.running_mean', 'features.4.0.block.1.1.running_var', 'features.4.0.block.1.1.num_batches_tracked', 'features.4.0.block.2.fc1.weight', 'features.4.0.block.2.fc1.bias', 'features.4.0.block.2.fc2.weight', 'features.4.0.block.2.fc2.bias', 'features.4.0.block.3.0.weight', 'features.4.0.block.3.1.weight', 'features.4.0.block.3.1.bias', 'features.4.0.block.3.1.running_mean', 'features.4.0.block.3.1.running_var', 'features.4.0.block.3.1.num_batches_tracked', 'features.4.1.block.0.0.weight', 'features.4.1.block.0.1.weight', 'features.4.1.block.0.1.bias', 'features.4.1.block.0.1.running_mean', 'features.4.1.block.0.1.running_var', 'features.4.1.block.0.1.num_batches_tracked', 'features.4.1.block.1.0.weight', 'features.4.1.block.1.1.weight', 'features.4.1.block.1.1.bias', 'features.4.1.block.1.1.running_mean', 'features.4.1.block.1.1.running_var', 'features.4.1.block.1.1.num_batches_tracked', 'features.4.1.block.2.fc1.weight', 'features.4.1.block.2.fc1.bias', 'features.4.1.block.2.fc2.weight', 'features.4.1.block.2.fc2.bias', 'features.4.1.block.3.0.weight', 'features.4.1.block.3.1.weight', 'features.4.1.block.3.1.bias', 'features.4.1.block.3.1.running_mean', 'features.4.1.block.3.1.running_var', 'features.4.1.block.3.1.num_batches_tracked', 'features.4.2.block.0.0.weight', 'features.4.2.block.0.1.weight', 'features.4.2.block.0.1.bias', 'features.4.2.block.0.1.running_mean', 'features.4.2.block.0.1.running_var', 'features.4.2.block.0.1.num_batches_tracked', 'features.4.2.block.1.0.weight', 'features.4.2.block.1.1.weight', 'features.4.2.block.1.1.bias', 'features.4.2.block.1.1.running_mean', 'features.4.2.block.1.1.running_var', 'features.4.2.block.1.1.num_batches_tracked', 'features.4.2.block.2.fc1.weight', 'features.4.2.block.2.fc1.bias', 'features.4.2.block.2.fc2.weight', 'features.4.2.block.2.fc2.bias', 'features.4.2.block.3.0.weight', 'features.4.2.block.3.1.weight', 'features.4.2.block.3.1.bias', 'features.4.2.block.3.1.running_mean', 'features.4.2.block.3.1.running_var', 'features.4.2.block.3.1.num_batches_tracked', 'features.5.0.block.0.0.weight', 'features.5.0.block.0.1.weight', 'features.5.0.block.0.1.bias', 'features.5.0.block.0.1.running_mean', 'features.5.0.block.0.1.running_var', 'features.5.0.block.0.1.num_batches_tracked', 'features.5.0.block.1.0.weight', 'features.5.0.block.1.1.weight', 'features.5.0.block.1.1.bias', 'features.5.0.block.1.1.running_mean', 'features.5.0.block.1.1.running_var', 'features.5.0.block.1.1.num_batches_tracked', 'features.5.0.block.2.fc1.weight', 'features.5.0.block.2.fc1.bias', 'features.5.0.block.2.fc2.weight', 'features.5.0.block.2.fc2.bias', 'features.5.0.block.3.0.weight', 'features.5.0.block.3.1.weight', 'features.5.0.block.3.1.bias', 'features.5.0.block.3.1.running_mean', 'features.5.0.block.3.1.running_var', 'features.5.0.block.3.1.num_batches_tracked', 'features.5.1.block.0.0.weight', 'features.5.1.block.0.1.weight', 'features.5.1.block.0.1.bias', 'features.5.1.block.0.1.running_mean', 'features.5.1.block.0.1.running_var', 'features.5.1.block.0.1.num_batches_tracked', 'features.5.1.block.1.0.weight', 'features.5.1.block.1.1.weight', 'features.5.1.block.1.1.bias', 'features.5.1.block.1.1.running_mean', 'features.5.1.block.1.1.running_var', 'features.5.1.block.1.1.num_batches_tracked', 'features.5.1.block.2.fc1.weight', 'features.5.1.block.2.fc1.bias', 'features.5.1.block.2.fc2.weight', 'features.5.1.block.2.fc2.bias', 'features.5.1.block.3.0.weight', 'features.5.1.block.3.1.weight', 'features.5.1.block.3.1.bias', 'features.5.1.block.3.1.running_mean', 'features.5.1.block.3.1.running_var', 'features.5.1.block.3.1.num_batches_tracked', 'features.5.2.block.0.0.weight', 'features.5.2.block.0.1.weight', 'features.5.2.block.0.1.bias', 'features.5.2.block.0.1.running_mean', 'features.5.2.block.0.1.running_var', 'features.5.2.block.0.1.num_batches_tracked', 'features.5.2.block.1.0.weight', 'features.5.2.block.1.1.weight', 'features.5.2.block.1.1.bias', 'features.5.2.block.1.1.running_mean', 'features.5.2.block.1.1.running_var', 'features.5.2.block.1.1.num_batches_tracked', 'features.5.2.block.2.fc1.weight', 'features.5.2.block.2.fc1.bias', 'features.5.2.block.2.fc2.weight', 'features.5.2.block.2.fc2.bias', 'features.5.2.block.3.0.weight', 'features.5.2.block.3.1.weight', 'features.5.2.block.3.1.bias', 'features.5.2.block.3.1.running_mean', 'features.5.2.block.3.1.running_var', 'features.5.2.block.3.1.num_batches_tracked', 'features.6.0.block.0.0.weight', 'features.6.0.block.0.1.weight', 'features.6.0.block.0.1.bias', 'features.6.0.block.0.1.running_mean', 'features.6.0.block.0.1.running_var', 'features.6.0.block.0.1.num_batches_tracked', 'features.6.0.block.1.0.weight', 'features.6.0.block.1.1.weight', 'features.6.0.block.1.1.bias', 'features.6.0.block.1.1.running_mean', 'features.6.0.block.1.1.running_var', 'features.6.0.block.1.1.num_batches_tracked', 'features.6.0.block.2.fc1.weight', 'features.6.0.block.2.fc1.bias', 'features.6.0.block.2.fc2.weight', 'features.6.0.block.2.fc2.bias', 'features.6.0.block.3.0.weight', 'features.6.0.block.3.1.weight', 'features.6.0.block.3.1.bias', 'features.6.0.block.3.1.running_mean', 'features.6.0.block.3.1.running_var', 'features.6.0.block.3.1.num_batches_tracked', 'features.6.1.block.0.0.weight', 'features.6.1.block.0.1.weight', 'features.6.1.block.0.1.bias', 'features.6.1.block.0.1.running_mean', 'features.6.1.block.0.1.running_var', 'features.6.1.block.0.1.num_batches_tracked', 'features.6.1.block.1.0.weight', 'features.6.1.block.1.1.weight', 'features.6.1.block.1.1.bias', 'features.6.1.block.1.1.running_mean', 'features.6.1.block.1.1.running_var', 'features.6.1.block.1.1.num_batches_tracked', 'features.6.1.block.2.fc1.weight', 'features.6.1.block.2.fc1.bias', 'features.6.1.block.2.fc2.weight', 'features.6.1.block.2.fc2.bias', 'features.6.1.block.3.0.weight', 'features.6.1.block.3.1.weight', 'features.6.1.block.3.1.bias', 'features.6.1.block.3.1.running_mean', 'features.6.1.block.3.1.running_var', 'features.6.1.block.3.1.num_batches_tracked', 'features.6.2.block.0.0.weight', 'features.6.2.block.0.1.weight', 'features.6.2.block.0.1.bias', 'features.6.2.block.0.1.running_mean', 'features.6.2.block.0.1.running_var', 'features.6.2.block.0.1.num_batches_tracked', 'features.6.2.block.1.0.weight', 'features.6.2.block.1.1.weight', 'features.6.2.block.1.1.bias', 'features.6.2.block.1.1.running_mean', 'features.6.2.block.1.1.running_var', 'features.6.2.block.1.1.num_batches_tracked', 'features.6.2.block.2.fc1.weight', 'features.6.2.block.2.fc1.bias', 'features.6.2.block.2.fc2.weight', 'features.6.2.block.2.fc2.bias', 'features.6.2.block.3.0.weight', 'features.6.2.block.3.1.weight', 'features.6.2.block.3.1.bias', 'features.6.2.block.3.1.running_mean', 'features.6.2.block.3.1.running_var', 'features.6.2.block.3.1.num_batches_tracked', 'features.6.3.block.0.0.weight', 'features.6.3.block.0.1.weight', 'features.6.3.block.0.1.bias', 'features.6.3.block.0.1.running_mean', 'features.6.3.block.0.1.running_var', 'features.6.3.block.0.1.num_batches_tracked', 'features.6.3.block.1.0.weight', 'features.6.3.block.1.1.weight', 'features.6.3.block.1.1.bias', 'features.6.3.block.1.1.running_mean', 'features.6.3.block.1.1.running_var', 'features.6.3.block.1.1.num_batches_tracked', 'features.6.3.block.2.fc1.weight', 'features.6.3.block.2.fc1.bias', 'features.6.3.block.2.fc2.weight', 'features.6.3.block.2.fc2.bias', 'features.6.3.block.3.0.weight', 'features.6.3.block.3.1.weight', 'features.6.3.block.3.1.bias', 'features.6.3.block.3.1.running_mean', 'features.6.3.block.3.1.running_var', 'features.6.3.block.3.1.num_batches_tracked', 'features.7.0.block.0.0.weight', 'features.7.0.block.0.1.weight', 'features.7.0.block.0.1.bias', 'features.7.0.block.0.1.running_mean', 'features.7.0.block.0.1.running_var', 'features.7.0.block.0.1.num_batches_tracked', 'features.7.0.block.1.0.weight', 'features.7.0.block.1.1.weight', 'features.7.0.block.1.1.bias', 'features.7.0.block.1.1.running_mean', 'features.7.0.block.1.1.running_var', 'features.7.0.block.1.1.num_batches_tracked', 'features.7.0.block.2.fc1.weight', 'features.7.0.block.2.fc1.bias', 'features.7.0.block.2.fc2.weight', 'features.7.0.block.2.fc2.bias', 'features.7.0.block.3.0.weight', 'features.7.0.block.3.1.weight', 'features.7.0.block.3.1.bias', 'features.7.0.block.3.1.running_mean', 'features.7.0.block.3.1.running_var', 'features.7.0.block.3.1.num_batches_tracked', 'features.8.0.weight', 'features.8.1.weight', 'features.8.1.bias', 'features.8.1.running_mean', 'features.8.1.running_var', 'features.8.1.num_batches_tracked', 'classifier.1.weight', 'classifier.1.bias'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4898/2552708146.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(model_path, map_location=torch.device(\"cpu\"))\n"
     ]
    }
   ],
   "source": [
    "state_dict = torch.load(model_path, map_location=torch.device(\"cpu\"))\n",
    "print(state_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "# Define transform for preprocessing\n",
    "data_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "def extract_features(image_path, model, transform):\n",
    "    \"\"\"\n",
    "    Extract feature embeddings from an image using the saved model.\n",
    "\n",
    "    Args:\n",
    "        image_path (str): Path to the image.\n",
    "        model (torch.nn.Module): Loaded model for feature extraction.\n",
    "        transform (callable): Data transform for preprocessing.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Feature embedding of the image.\n",
    "    \"\"\"\n",
    "    device = next(model.parameters()).device\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    image = transform(image).unsqueeze(0).to(device)  # Add batch dimension\n",
    "\n",
    "    with torch.no_grad():\n",
    "        features = model(image).squeeze().cpu().numpy()\n",
    "\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def find_most_similar(input_image_path, valid_images_folder, model, transform, threshold=0.8):\n",
    "    \"\"\"\n",
    "    Compare an input image against valid images to find the most similar one.\n",
    "\n",
    "    Args:\n",
    "        input_image_path (str): Path to the input image.\n",
    "        valid_images_folder (str): Path to the folder containing valid images.\n",
    "        model (torch.nn.Module): Loaded model for feature extraction.\n",
    "        transform (callable): Data transform for preprocessing.\n",
    "        threshold (float): Similarity threshold for classification.\n",
    "\n",
    "    Returns:\n",
    "        str, float: Most similar valid image and similarity score, or \"not similar\".\n",
    "    \"\"\"\n",
    "    # Extract features for the input image\n",
    "    input_features = extract_features(input_image_path, model, transform)\n",
    "\n",
    "    most_similar_image = None\n",
    "    highest_similarity = -1\n",
    "\n",
    "    # Loop through valid images\n",
    "    for valid_image_name in os.listdir(valid_images_folder):\n",
    "        valid_image_path = os.path.join(valid_images_folder, valid_image_name)\n",
    "        \n",
    "        # Skip non-image files\n",
    "        if not valid_image_name.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif', '.svg')):\n",
    "            continue\n",
    "        \n",
    "        # Extract features for the valid image\n",
    "        valid_features = extract_features(valid_image_path, model, transform)\n",
    "\n",
    "        # Compute cosine similarity\n",
    "        similarity = cosine_similarity([input_features], [valid_features])[0][0]\n",
    "\n",
    "        # Update the most similar image if the current similarity is higher\n",
    "        if similarity > highest_similarity:\n",
    "            highest_similarity = similarity\n",
    "            most_similar_image = valid_image_name\n",
    "\n",
    "    # Decision based on threshold\n",
    "    if highest_similarity >= threshold:\n",
    "        return most_similar_image, highest_similarity\n",
    "    else:\n",
    "        return \"not similar\", highest_similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input image is similar to BM_LOGO-03.png with similarity score: 0.9995\n"
     ]
    }
   ],
   "source": [
    "input_image_path = \"/path/to/input/image.jpg\"\n",
    "# valid_images_folder = \"/path/to/valid/images\"\n",
    "threshold = 0.8\n",
    "\n",
    "img1 = 'test_2/302_29.jpg'\n",
    "\n",
    "# valid_img = ['BM_LOGO-00.png' , 'BM_LOGO-01.png' ,  'BM_LOGO-02.png' , 'BM_LOGO-03.png' , 'BM_LOGO-04.png','BM_LOGO-05.png']\n",
    "input_image_path = f\"/home/mahdi/Phishing_Project/images/{img1}\"\n",
    "\n",
    "# valid_img_path = '/home/mahdi/Phishing_Project/Valid_images/'\n",
    "\n",
    "# input_image_path = \"/path/to/input/logo.jpg\"\n",
    "valid_images_folder = '/home/mahdi/Phishing_Project/Valid_images'\n",
    "\n",
    "\n",
    "\n",
    "result, similarity_score = find_most_similar(input_image_path, valid_images_folder, model, data_transform, threshold)\n",
    "if result != \"not similar\":\n",
    "    print(f\"Input image is similar to {result} with similarity score: {similarity_score:.4f}\")\n",
    "else:\n",
    "    print(f\"Input image is not similar to any valid image with similarity score: {similarity_score:.4f}.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
